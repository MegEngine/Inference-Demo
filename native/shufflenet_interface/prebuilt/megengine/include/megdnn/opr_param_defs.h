// generated by gen_param_defs.py for 3ee22bd53ea17a1bc58e7b12d5bea747583fa311cd23cab1a3a07733cf74642f
#pragma once
#include "megdnn/dtype.h"
#include <stdint.h>
#include <string.h>
namespace megdnn {
namespace param {
struct Empty {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3334509756u;
};

struct Axis {
    static MEGDNN_CONSTEXPR uint32_t TAG = 850869422u;
    union { struct {
    int32_t axis;
    }; };
    Axis(int32_t axis_=0) {
        memset(this, 0, sizeof(*this));
        this->axis = axis_;
    }
};

struct ConvolutionV0 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2883667579u;
    enum class Mode: uint32_t {
        CROSS_CORRELATION = 0,
        CONVOLUTION = 1
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    enum class DataType: uint32_t {
        //! input/output both float32/float16
        FLOAT = 0,
        INT8x8x16 = 1,
        INT8x8x32 = 2,
        //! input/output both float16, the internal compute is float32
        FLOAT_IO16xC32 = 3,
        //! input QuantizedAsymm8, output QuantizedS32
        QUINT8x8x32 = 4,
        //! input int8, output specified by tensor DType
        INT8x8xX = 5,
        //! input QuantizedAsymm4, output QuantizedS32
        QUINT4x4x32 = 6
    };
    static MEGDNN_CONSTEXPR uint32_t DATATYPE_NR_MEMBER = 7;
    enum class Sparse: uint32_t {
        /*!
         * dense convolution: filter shape should be [oc, ic, spatial...] if
         * format is NCHW, [oc, spatial..., ic] if format is NHWC
         */
        DENSE = 0,
        /*!
         * group convolution: filter shape should be [group, oc_per_group,
         * ic_per_group, spatial...] if format is NCHW, [group, oc_per_group,
         * spatial..., ic_per_group] if format is NHWC
         */
        GROUP = 1
    };
    static MEGDNN_CONSTEXPR uint32_t SPARSE_NR_MEMBER = 2;
    /*!
     * convolution data/filter/output format; see :class:`RelayoutFormat` for
     * more details
     */
    enum class Format: uint32_t {
        NCHW = 0,
        NHWC = 1,
        NHWCD4 = 2,
        NCHW4 = 3,
        NCHW8 = 4,
        NCHW32 = 5,
        NCHW88 = 6,
        NCHW44 = 7,
        NCHW44_DOT = 8,
        //! NCHW layout with weights tranformed by winograd
        NCHW_WINOGRAD = 9,
        //! NCHW88 layout with weights tranformed by winograd
        NCHW88_WINOGRAD = 10,
        //! NCHW44 layout with weights tranformed by winograd
        NCHW44_WINOGRAD = 11,
        /*!
         * CHWN4 is currently only used on Nvidia platform for fast
         * implementation of convolution using CUDA/SASS. The channels are
         * splitted to groups of 4 channels.
         */
        CHWN4 = 12
    };
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    Mode mode;
    //! padding on one side on the first dimension
    uint32_t pad_h;
    //! padding on one side on the second dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_h;
    //! kernel stride on the second dimension
    uint32_t stride_w;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_h;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_w;
    DataType data_type;
    Sparse sparse;
    Format format;
    }; };
    ConvolutionV0(Mode mode_=Mode::CROSS_CORRELATION, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1, DataType data_type_=DataType::FLOAT, Sparse sparse_=Sparse::DENSE, Format format_=Format::NCHW) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
        this->data_type = data_type_;
        this->sparse = sparse_;
        this->format = format_;
    }
};

struct Convolution {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3401828963u;
    using Mode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    using Sparse = ConvolutionV0::Sparse;
    static MEGDNN_CONSTEXPR uint32_t SPARSE_NR_MEMBER = 2;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    /*!
     * Specifies special computation modes, e.g. different combinations of
     * intermediate result data types.
     */
    enum class ComputeMode: uint32_t {
        //! No special requirements on the precision of intermediate results.
        DEFAULT = 0,
        /*!
         * Use Float32 accumulator and intermediate result. Only supported when
         * input and output is Float16.
         */
        FLOAT32 = 1
    };
    static MEGDNN_CONSTEXPR uint32_t COMPUTEMODE_NR_MEMBER = 2;
    union { struct {
    Mode mode;
    //! padding on one side on the first dimension
    uint32_t pad_h;
    //! padding on one side on the second dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_h;
    //! kernel stride on the second dimension
    uint32_t stride_w;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_h;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_w;
    Sparse sparse;
    Format format;
    ComputeMode compute_mode;
    }; };
    Convolution(Mode mode_=Mode::CROSS_CORRELATION, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1, Sparse sparse_=Sparse::DENSE, Format format_=Format::NCHW, ComputeMode compute_mode_=ComputeMode::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
        this->sparse = sparse_;
        this->format = format_;
        this->compute_mode = compute_mode_;
    }
};

struct MaskPropagate {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3985030427u;
    union { struct {
    //! padding on one side on the first dimension
    uint32_t pad_h;
    //! padding on one side on the second dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_h;
    //! kernel stride on the second dimension
    uint32_t stride_w;
    //! kernel height
    uint32_t kernel_h;
    //! kernel width
    uint32_t kernel_w;
    //! dilate height
    uint32_t dilate_h;
    //! dilate width
    uint32_t dilate_w;
    }; };
    MaskPropagate(uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t kernel_h_=1, uint32_t kernel_w_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1) {
        memset(this, 0, sizeof(*this));
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->kernel_h = kernel_h_;
        this->kernel_w = kernel_w_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
    }
};

struct ConvPooling {
    static MEGDNN_CONSTEXPR uint32_t TAG = 937489154u;
    enum class Method: uint32_t {
        WITH_TEXTURE_OBJ = 0,
        WITH_SHARED_MEM = 1
    };
    static MEGDNN_CONSTEXPR uint32_t METHOD_NR_MEMBER = 2;
    using ConvMode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t CONVMODE_NR_MEMBER = 2;
    enum class PoolMode: uint32_t {
        AVERAGE = 0,
        MAX = 1
    };
    static MEGDNN_CONSTEXPR uint32_t POOLMODE_NR_MEMBER = 2;
    enum class NonlineMode: uint32_t {
        IDENTITY = 0,
        RELU = 1,
        SIGMOID = 2
    };
    static MEGDNN_CONSTEXPR uint32_t NONLINEMODE_NR_MEMBER = 3;
    union { struct {
    Method method;
    ConvMode convMode;
    PoolMode poolMode;
    NonlineMode nonlineMode;
    uint32_t pool_shape_h;
    uint32_t pool_shape_w;
    uint32_t pool_stride_h;
    uint32_t pool_stride_w;
    uint32_t pool_pad_h;
    uint32_t pool_pad_w;
    uint32_t conv_stride_h;
    uint32_t conv_stride_w;
    uint32_t conv_pad_h;
    uint32_t conv_pad_w;
    }; };
    ConvPooling(Method method_=Method::WITH_TEXTURE_OBJ, ConvMode convMode_=ConvMode::CROSS_CORRELATION, PoolMode poolMode_=PoolMode::AVERAGE, NonlineMode nonlineMode_=NonlineMode::IDENTITY, uint32_t pool_shape_h_=1, uint32_t pool_shape_w_=1, uint32_t pool_stride_h_=1, uint32_t pool_stride_w_=1, uint32_t pool_pad_h_=0, uint32_t pool_pad_w_=0, uint32_t conv_stride_h_=1, uint32_t conv_stride_w_=1, uint32_t conv_pad_h_=0, uint32_t conv_pad_w_=0) {
        memset(this, 0, sizeof(*this));
        this->method = method_;
        this->convMode = convMode_;
        this->poolMode = poolMode_;
        this->nonlineMode = nonlineMode_;
        this->pool_shape_h = pool_shape_h_;
        this->pool_shape_w = pool_shape_w_;
        this->pool_stride_h = pool_stride_h_;
        this->pool_stride_w = pool_stride_w_;
        this->pool_pad_h = pool_pad_h_;
        this->pool_pad_w = pool_pad_w_;
        this->conv_stride_h = conv_stride_h_;
        this->conv_stride_w = conv_stride_w_;
        this->conv_pad_h = conv_pad_h_;
        this->conv_pad_w = conv_pad_w_;
    }
};

//! legacy conv_bias
struct ConvBiasV0 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1139437340u;
    enum class NonlineMode: uint32_t {
        IDENTITY = 0,
        RELU = 1,
        SIGMOID = 2,
        H_SWISH = 3
    };
    static MEGDNN_CONSTEXPR uint32_t NONLINEMODE_NR_MEMBER = 4;
    using Mode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    union { struct {
    NonlineMode nonlineMode;
    Mode mode;
    uint32_t pad_h;
    uint32_t pad_w;
    uint32_t stride_h;
    uint32_t stride_w;
    }; };
    ConvBiasV0(NonlineMode nonlineMode_=NonlineMode::IDENTITY, Mode mode_=Mode::CROSS_CORRELATION, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1) {
        memset(this, 0, sizeof(*this));
        this->nonlineMode = nonlineMode_;
        this->mode = mode_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
    }
};

//! active(conv(x, w) + bias)
struct ConvBiasV1 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3097296471u;
    using NonlineMode = ConvBiasV0::NonlineMode;
    static MEGDNN_CONSTEXPR uint32_t NONLINEMODE_NR_MEMBER = 4;
    using Mode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    using DataType = ConvolutionV0::DataType;
    static MEGDNN_CONSTEXPR uint32_t DATATYPE_NR_MEMBER = 7;
    using Sparse = ConvolutionV0::Sparse;
    static MEGDNN_CONSTEXPR uint32_t SPARSE_NR_MEMBER = 2;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    NonlineMode nonlineMode;
    Mode mode;
    DataType data_type;
    Sparse sparse;
    Format format;
    //! padding on one side on the first dimension
    uint32_t pad_h;
    //! padding on one side on the second dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_h;
    //! kernel stride on the second dimension
    uint32_t stride_w;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_h;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_w;
    }; };
    ConvBiasV1(NonlineMode nonlineMode_=NonlineMode::IDENTITY, Mode mode_=Mode::CROSS_CORRELATION, DataType data_type_=DataType::FLOAT, Sparse sparse_=Sparse::DENSE, Format format_=Format::NCHW, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1) {
        memset(this, 0, sizeof(*this));
        this->nonlineMode = nonlineMode_;
        this->mode = mode_;
        this->data_type = data_type_;
        this->sparse = sparse_;
        this->format = format_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
    }
};

//! active(conv(x, w) + bias)
struct ConvBiasV2 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3865950583u;
    using NonlineMode = ConvBiasV0::NonlineMode;
    static MEGDNN_CONSTEXPR uint32_t NONLINEMODE_NR_MEMBER = 4;
    using Mode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    using Sparse = ConvolutionV0::Sparse;
    static MEGDNN_CONSTEXPR uint32_t SPARSE_NR_MEMBER = 2;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    using ComputeMode = Convolution::ComputeMode;
    static MEGDNN_CONSTEXPR uint32_t COMPUTEMODE_NR_MEMBER = 2;
    union { struct {
    NonlineMode nonlineMode;
    Mode mode;
    Sparse sparse;
    Format format;
    //! padding on one side on the first dimension
    uint32_t pad_h;
    //! padding on one side on the second dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_h;
    //! kernel stride on the second dimension
    uint32_t stride_w;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_h;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_w;
    ComputeMode compute_mode;
    }; };
    ConvBiasV2(NonlineMode nonlineMode_=NonlineMode::IDENTITY, Mode mode_=Mode::CROSS_CORRELATION, Sparse sparse_=Sparse::DENSE, Format format_=Format::NCHW, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1, ComputeMode compute_mode_=ComputeMode::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->nonlineMode = nonlineMode_;
        this->mode = mode_;
        this->sparse = sparse_;
        this->format = format_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
        this->compute_mode = compute_mode_;
    }
};

//! active(conv(x, w) + bias)
struct ConvBias {
    static MEGDNN_CONSTEXPR uint32_t TAG = 631167930u;
    using NonlineMode = ConvBiasV0::NonlineMode;
    static MEGDNN_CONSTEXPR uint32_t NONLINEMODE_NR_MEMBER = 4;
    using Mode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    using Sparse = ConvolutionV0::Sparse;
    static MEGDNN_CONSTEXPR uint32_t SPARSE_NR_MEMBER = 2;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    using ComputeMode = Convolution::ComputeMode;
    static MEGDNN_CONSTEXPR uint32_t COMPUTEMODE_NR_MEMBER = 2;
    union { struct {
    NonlineMode nonlineMode;
    Mode mode;
    Sparse sparse;
    Format format;
    //! padding on one side on the first dimension
    uint32_t pad_h;
    //! padding on one side on the second dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_h;
    //! kernel stride on the second dimension
    uint32_t stride_w;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_h;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_w;
    //! detail meaning \see winograd in conv bias
    uint32_t output_block_size;
    ComputeMode compute_mode;
    }; };
    ConvBias(NonlineMode nonlineMode_=NonlineMode::IDENTITY, Mode mode_=Mode::CROSS_CORRELATION, Sparse sparse_=Sparse::DENSE, Format format_=Format::NCHW, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1, uint32_t output_block_size_=0, ComputeMode compute_mode_=ComputeMode::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->nonlineMode = nonlineMode_;
        this->mode = mode_;
        this->sparse = sparse_;
        this->format = format_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
        this->output_block_size = output_block_size_;
        this->compute_mode = compute_mode_;
    }
};

struct SeparableConv {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2073960474u;
    using Mode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    enum class BorderMode: uint32_t {
        BORDER_REPLICATE = 0,
        BORDER_REFLECT = 1,
        BORDER_REFLECT_101 = 2,
        BORDER_WRAP = 3,
        BORDER_CONSTANT = 4,
        BORDER_TRANSPARENT = 5,
        BORDER_ISOLATED = 6
    };
    static MEGDNN_CONSTEXPR uint32_t BORDERMODE_NR_MEMBER = 7;
    union { struct {
    Mode mode;
    BorderMode borderMode;
    bool is_symm_kernel;
    uint32_t pad_h;
    uint32_t pad_w;
    uint32_t stride_h;
    uint32_t stride_w;
    uint32_t ksize_h;
    uint32_t ksize_w;
    uint32_t anchor_h;
    uint32_t anchor_w;
    }; };
    SeparableConv(Mode mode_=Mode::CROSS_CORRELATION, BorderMode borderMode_=BorderMode::BORDER_REPLICATE, bool is_symm_kernel_=true, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t ksize_h_=3, uint32_t ksize_w_=3, uint32_t anchor_h_=1, uint32_t anchor_w_=1) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->borderMode = borderMode_;
        this->is_symm_kernel = is_symm_kernel_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->ksize_h = ksize_h_;
        this->ksize_w = ksize_w_;
        this->anchor_h = anchor_h_;
        this->anchor_w = anchor_w_;
    }
};

struct Images2Neibs {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3022653556u;
    union { struct {
    uint32_t pad_h;
    uint32_t pad_w;
    uint32_t stride_h;
    uint32_t stride_w;
    uint32_t window_h;
    uint32_t window_w;
    }; };
    Images2Neibs(uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t window_h_=3, uint32_t window_w_=3) {
        memset(this, 0, sizeof(*this));
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->window_h = window_h_;
        this->window_w = window_w_;
    }
};

struct Pooling {
    static MEGDNN_CONSTEXPR uint32_t TAG = 581590217u;
    enum class Mode: uint32_t {
        //! maximum value inside pooling window
        MAX = 0,
        /*!
         * arithmetic mean of all values inside pooling window. Padding values
         * are taken into account and are viewed as zero
         */
        AVERAGE = 1,
        /*!
         * arithmetic mean of all values inside pooling window. No padding
         * isused.
         */
        AVERAGE_COUNT_EXCLUDE_PADDING = 2
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 3;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    Mode mode;
    uint32_t pad_h;
    uint32_t pad_w;
    uint32_t stride_h;
    uint32_t stride_w;
    uint32_t window_h;
    uint32_t window_w;
    Format format;
    }; };
    Pooling(Mode mode_=Mode::MAX, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=2, uint32_t stride_w_=2, uint32_t window_h_=2, uint32_t window_w_=2, Format format_=Format::NCHW) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->window_h = window_h_;
        this->window_w = window_w_;
        this->format = format_;
    }
};

struct AdaptivePooling {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3024348844u;
    using Mode = Pooling::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 3;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    Mode mode;
    Format format;
    }; };
    AdaptivePooling(Mode mode_=Mode::MAX, Format format_=Format::NCHW) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->format = format_;
    }
};

/*!
 * see ImageNet Classification with Deep Convolutional Neural Networks for
 * meaning of the fields
 */
struct LRN {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2388216219u;
    union { struct {
    //! must be odd
    uint32_t n;
    float k;
    float alpha;
    float beta;
    }; };
    LRN(uint32_t n_=5, float k_=2.f, float alpha_=1e-4f, float beta_=0.75f) {
        memset(this, 0, sizeof(*this));
        this->n = n_;
        this->k = k_;
        this->alpha = alpha_;
        this->beta = beta_;
    }
};

struct BN {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3765986029u;
    enum class ParamDim: uint32_t {
        //! Dim of params (Sigma, Mu) is 1 x 1 x H x W
        DIM_11HW = 0,
        //! Dim of params (Sigma, Mu) is 1 x C x H x W
        DIM_1CHW = 1,
        //! Dim of params (Sigma, Mu) is 1 x C x 1 x 1
        DIM_1C11 = 2
    };
    static MEGDNN_CONSTEXPR uint32_t PARAMDIM_NR_MEMBER = 3;
    enum class FwdMode: uint32_t {
        //! Training phase.
        TRAINING = 0,
        //! Inference phase.
        INFERENCE = 1
    };
    static MEGDNN_CONSTEXPR uint32_t FWDMODE_NR_MEMBER = 2;
    union { struct {
    ParamDim param_dim;
    FwdMode fwd_mode;
    double epsilon;
    double avg_factor;
    float scale;
    float bias;
    }; };
    BN(ParamDim param_dim_=ParamDim::DIM_11HW, FwdMode fwd_mode_=FwdMode::TRAINING, double epsilon_=1e-4f, double avg_factor_=1.f, float scale_=1.f, float bias_=0.f) {
        memset(this, 0, sizeof(*this));
        this->param_dim = param_dim_;
        this->fwd_mode = fwd_mode_;
        this->epsilon = epsilon_;
        this->avg_factor = avg_factor_;
        this->scale = scale_;
        this->bias = bias_;
    }
};

struct ROIPooling {
    static MEGDNN_CONSTEXPR uint32_t TAG = 111572365u;
    enum class Mode: uint32_t {
        /*!
         * maximum value inside pooling window; pooling result would be 0 if
         * pooling window is empty
         */
        MAX = 0,
        /*!
         * arithmetic mean of all values inside pooling window; pooling result
         * would be 0 if pooling window is empty
         */
        AVERAGE = 1
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    union { struct {
    Mode mode;
    float scale;
    }; };
    ROIPooling(Mode mode_=Mode::MAX, float scale_=1.f) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->scale = scale_;
    }
};

struct WarpPerspective {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3222348414u;
    enum class InterpolationMode: uint32_t {
        NEAREST = 0,
        LINEAR = 1,
        AREA = 2,
        CUBIC = 3,
        LANCZOS4 = 4,
        INTER_NEAREST = NEAREST,
        INTER_LINEAR = LINEAR,
        INTER_AREA = AREA,
        INTER_CUBIC = CUBIC,
        INTER_LANCZOS4 = LANCZOS4,
    };
    static MEGDNN_CONSTEXPR uint32_t INTERPOLATIONMODE_NR_MEMBER = 5;
    enum class BorderMode: uint32_t {
        //! aaaaaa|abcdefgh|hhhhhhh
        REPLICATE = 0,
        //! fedcba|abcdefgh|hgfedcb
        REFLECT = 1,
        //! gfedcb|abcdefgh|gfedcba
        REFLECT_101 = 2,
        //! cdefgh|abcdefgh|abcdefg
        WRAP = 3,
        //! iiiiii|abcdefgh|iiiiiii
        CONSTANT = 4,
        TRANSPARENT = 5,
        ISOLATED = 6,
        BORDER_REPLICATE = REPLICATE,
        BORDER_REFLECT = REFLECT,
        BORDER_REFLECT_101 = REFLECT_101,
        BORDER_WRAP = WRAP,
        BORDER_CONSTANT = CONSTANT,
        BORDER_TRANSPARENT = TRANSPARENT,
        BORDER_ISOLATED = ISOLATED,
    };
    static MEGDNN_CONSTEXPR uint32_t BORDERMODE_NR_MEMBER = 7;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    InterpolationMode imode;
    BorderMode bmode;
    Format format;
    //! used for CONSTANT bmode
    float border_val;
    }; };
    WarpPerspective(InterpolationMode imode_=InterpolationMode::LINEAR, BorderMode bmode_=BorderMode::REPLICATE, Format format_=Format::NCHW, float border_val_=.0f) {
        memset(this, 0, sizeof(*this));
        this->imode = imode_;
        this->bmode = bmode_;
        this->format = format_;
        this->border_val = border_val_;
    }
};

struct SpatialTfGridGenerator {
    static MEGDNN_CONSTEXPR uint32_t TAG = 596233615u;
    enum class Mode: uint32_t {
        AFFINE = 0
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 1;
    union { struct {
    Mode mode;
    }; };
    SpatialTfGridGenerator(Mode mode_=Mode::AFFINE) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
    }
};

struct SpatialTfSampler {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3856477188u;
    enum class Mode: uint32_t {
        BILINEAR = 0
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 1;
    union { struct {
    Mode mode;
    }; };
    SpatialTfSampler(Mode mode_=Mode::BILINEAR) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
    }
};

struct AddUpdate {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1017839835u;
    union { struct {
    float alpha;
    float beta;
    float bias;
    }; };
    AddUpdate(float alpha_=1.f, float beta_=1.f, float bias_=0.f) {
        memset(this, 0, sizeof(*this));
        this->alpha = alpha_;
        this->beta = beta_;
        this->bias = bias_;
    }
};

struct Elemwise {
    static MEGDNN_CONSTEXPR uint32_t TAG = 791173831u;
    enum class Mode: uint32_t {
        //! unary: max(x, 0)
        RELU = 0,
        //! unary: abs(x)
        ABS = 1,
        //! unary: acos(x)
        ACOS = 2,
        //! unary: asin(x)
        ASIN = 3,
        //! unary: ceil(x)
        CEIL = 4,
        //! unary: cos(x)
        COS = 5,
        //! unary: exp(x)
        EXP = 6,
        //! unary: numerically stable exp(x)-1
        EXPM1 = 7,
        //! unary: floor(x)
        FLOOR = 8,
        //! unary: natural logarithm, log(x)
        LOG = 9,
        //! unary: numerically stable log(x+1)
        LOG1P = 10,
        //! unary: -x
        NEGATE = 11,
        //! unary: 1/(1+exp(-x))
        SIGMOID = 12,
        //! unary: sin(x)
        SIN = 13,
        //! unary: tanh(x)
        TANH = 14,
        //! binary: x > 0 ? y : -y
        ABS_GRAD = 15,
        //! binary: x + y
        ADD = 16,
        //! binary: floor(x / y)
        FLOOR_DIV = 17,
        //! binary: max(x, y)
        MAX = 18,
        //! binary: min(x, y)
        MIN = 19,
        //! binary: x % y or fmodf(x, y)
        MOD = 20,
        //! binary: x * y
        MUL = 21,
        //! binary: pow(x, y)
        POW = 22,
        //! binary: x * (1 - x) * y
        SIGMOID_GRAD = 23,
        //! binary: x - y
        SUB = 24,
        //! binary: (x > 0) * y
        SWITCH_GT0 = 25,
        //! binary: (1 - x * x) * y
        TANH_GRAD = 26,
        //! binary: x / y
        TRUE_DIV = 27,
        //! binary: numerically stable log(exp(x) + exp(y))
        LOG_SUM_EXP = 28,
        //! binary: x < y
        LT = 29,
        //! binary: x <= y
        LEQ = 30,
        //! binary: x == y
        EQ = 31,
        /*!
         * bitwise binary: x << y. Note that result is undefined if y < 0 or y
         * >= bitwidth. Logical shift is performed for unsigned intergers, and
         * arithmetic shift for signed ones.
         */
        SHL = 32,
        //! bitwise binary: x >> y; see SHL mode for more details
        SHR = 33,
        //! ternary: x <= y ? z : 0
        COND_LEQ_MOV = 34,
        /*!
         * compute ``a * b + c`` where c must either have same layout as a or
         * b, or be a scalar
         */
        FUSE_MUL_ADD3 = 35,
        /*!
         * compute ``a * A + b * B`` where a and b must have equal layout, and
         * A and B must have equal layout. In the inputs ``b`` and ``B`` can be
         * swapped
         */
        FUSE_MUL_ADD4 = 36,
        //! binary: max(x+y, 0)
        FUSE_ADD_RELU = 37,
        //! binary: 1/(1+exp(-(x+y)))
        FUSE_ADD_SIGMOID = 38,
        //! binary: tanh(x+y)
        FUSE_ADD_TANH = 39,
        //! unary: rational approximation of tanh(x)
        FAST_TANH = 40,
        //! binary: grad of the rational approximation of tanh(x)
        FAST_TANH_GRAD = 41,
        /*!
         * unary: round(x), the nearest integer value to x, rounding halfway
         * cases away from zero. Float only.
         */
        ROUND = 42,
        /*!
         * binary: rounded higher l bits of x * y, where l is the bit length of
         * x.
         */
        RMULH = 43,
        //! binary: atan2(y,x)
        ATAN2 = 44,
        //! unary: erf(x)
        ERF = 45,
        //! unary: inverse function of erf(x)
        ERFINV = 46,
        //! unary: erfc(x)
        ERFC = 47,
        //! unary: inverse function of erfc(x)
        ERFCINV = 48,
        //! unary: x * clip(x + 3, 0, 6) / 6
        H_SWISH = 49,
        //! binary: x < -3 ? 0 : (x > 3 ? y : (2 * x + 3) / 6 * y)
        H_SWISH_GRAD = 50,
        //! binary: hswish(x+y)
        FUSE_ADD_H_SWISH = 51,
        //! unary: !x
        NOT = 52,
        //! binary: x && y
        AND = 53,
        //! binary: x || y
        OR = 54,
        //! binary: x ^ y
        XOR = 55
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 56;
    union { struct {
    Mode mode;
    }; };
    Elemwise(Mode mode_=Mode::RELU) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
    }
};

struct ElemwiseMultiType {
    static MEGDNN_CONSTEXPR uint32_t TAG = 139916022u;
    enum class Mode: uint32_t {
        /*!
         * compute ``a * b + c`` requiring that ``a`` be int16 and ``b`` and
         * ``c``  int32, and the result is int32. This mode is optimized for
         * the channel-broadacsted case, i.e. ``a`` has shape (A, B, C) and
         * ``b`` and ``c`` have shape (1, C, 1)
         */
        FUSE_MUL_ADD3_INT16x32x32x32 = 0,
        /*!
         * compuate ``a * b + c`` where the inputs ``a`` is an integer type
         * ``b`` and ``c`` are both ``float32``, the result is ``int8``. This
         * is currently only optimized for ``(1, x)`` broadcast for ``b`` and
         * ``c``. Computation is carried in floating points and results are
         * rounded towards zero with saturated cast to int.
         */
        FUSE_MUL_ADD3_IXxF32xF32xI8 = 1,
        /*!
         * Compute ``a >> b``, round the result according to lower ``b`` bits
         * of ``a``` and make a saturating conversion to int8. Where ``a``
         * should be an integer tensor and ``b`` should be an int8 scalar.
         */
        ROUND_SHR_SATURATE_IXxI8xI8 = 2,
        /*!
         * Fused operation of an int16 elemwise add, an int16 rounding multiply
         * high and an int16 to int8 rounding right shift with saturation.
         */
        FUSE_ADD_RMULH_ROUND_SHR_SATURATE_INT16x16x16x8 = 3,
        /*!
         * Fused operation of an int32 elemwise add, an int32 rounding multiply
         * high and an int32 to int8 rounding right shift with saturation.
         */
        FUSE_ADD_RMULH_ROUND_SHR_SATURATE_INT32x32x32x8 = 4,
        /*!
         * Compute ``a >> b``, round the result according to lower ``b`` bits
         * of ``a``` and make a saturating conversion to int16. Where ``a``
         * should be an integer tensor and ``b`` should be an int8 scalar.
         */
        ROUND_SHR_SATURATE_IXxI8xI16 = 5,
        /*!
         * Fused elemwise add two quantized int8 with specifiedoutput quantized
         * dtype
         */
        QADD = 6,
        /*!
         * Fused elemwise add two quantized int8 followed by ReLU and typecvt
         * to specified dtype
         */
        QFUSE_ADD_RELU = 7,
        /*!
         * Fused elemwise multiply two quantized int8 with specifiedoutput
         * quantized dtype
         */
        QMUL = 8,
        /*!
         * Fused elemwise min two quantized int8 with specifiedoutput quantized
         * dtype
         */
        QMIN = 9,
        //! quantized: max(x, y), with specified output quantized dtype
        QMAX = 10,
        //! quantized: x - y
        QSUB = 11,
        //! quantized: x / y
        QTRUE_DIV = 12,
        //! quantized: sigmoid(x + y)
        QFUSE_ADD_SIGMOID = 13,
        //! quantized: tanh(x + y)
        QFUSE_ADD_TANH = 14,
        //! quantized: x > 0 ? x : 0
        QRELU = 15,
        //! quantized: x > 0 ? x : -x
        QABS = 16,
        //! quantized: sigmoid(x)
        QSIGMOID = 17,
        //! quantized: exp(x)
        QEXP = 18,
        //! quantized: tanh(x)
        QTANH = 19,
        //! quantized: x * y + z
        QFUSE_MUL_ADD3 = 20,
        //! quantized: fast_tanh(x)
        QFAST_TANH = 21,
        //! quantized: -x
        QNEGATE = 22,
        //! quantized: acos(x)
        QACOS = 23,
        //! quantized: asin(x)
        QASIN = 24,
        //! quantized: ceil(x)
        QCEIL = 25,
        //! quantized: cos(x)
        QCOS = 26,
        //! quantized: expm1(x)
        QEXPM1 = 27,
        //! quantized: floor(x)
        QFLOOR = 28,
        //! quantized: log(x)
        QLOG = 29,
        //! quantized: log1p(x)
        QLOG1P = 30,
        //! quantized: sin(x)
        QSIN = 31,
        //! quantized: round(x)
        QROUND = 32,
        //! quantized: erf(x)
        QERF = 33,
        //! quantized: erfinv(x)
        QERFINV = 34,
        //! quantized: erfc(x)
        QERFC = 35,
        //! quantized: erfcinv(x)
        QERFCINV = 36,
        //! quantized: abs_grad
        QABS_GRAD = 37,
        //! quantized floor_div
        QFLOOR_DIV = 38,
        //! quantized mod
        QMOD = 39,
        //! quantized sigmoid_grad
        QSIGMOID_GRAD = 40,
        //! quantized switch_gt0
        QSWITCH_GT0 = 41,
        //! quantized tanh_grad
        QTANH_GRAD = 42,
        //! quantized lt
        QLT = 43,
        //! quantized leq
        QLEQ = 44,
        //! quantized eq
        QEQ = 45,
        //! quantized pow
        QPOW = 46,
        //! quantized log_sum_exp
        QLOG_SUM_EXP = 47,
        //! quantized fast_tanh_grad
        QFAST_TANH_GRAD = 48,
        //! quantized atan2
        QATAN2 = 49,
        //! quantized cond_leq_mov
        QCOND_LEQ_MOV = 50,
        //! quantized h_swish
        QH_SWISH = 51,
        //! quantized h_swish(x+y)
        QFUSE_ADD_H_SWISH = 52,
        //! quantized h_swish_grad
        QH_SWISH_GRAD = 53
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 54;
    union { struct {
    Mode mode;
    }; };
    ElemwiseMultiType(Mode mode_=Mode::FUSE_MUL_ADD3_INT16x32x32x32) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
    }
};

//! power with constant exponent
struct PowC {
    static MEGDNN_CONSTEXPR uint32_t TAG = 688442896u;
    union { struct {
    float exp;
    }; };
    PowC(float exp_=0) {
        memset(this, 0, sizeof(*this));
        this->exp = exp_;
    }
};

struct MatrixMulV0 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2382437341u;
    enum class DataType: uint32_t {
        //! input/output both float32/float16
        FLOAT = 0,
        INT8x8x16 = 1,
        INT8x8x32 = 2,
        //! input/output both float16, the internal compute is float32
        FLOAT_IO16xC32 = 3,
        //! input QuantizedAsymm8, output QuantizedS32
        QUINT8x8x32 = 4,
        //! input QuantizedAsymm4, output QuantizedS32
        QUINT4x4x32 = 5
    };
    static MEGDNN_CONSTEXPR uint32_t DATATYPE_NR_MEMBER = 6;
    union { struct {
    bool transposeA;
    bool transposeB;
    DataType data_type;
    }; };
    MatrixMulV0(bool transposeA_=false, bool transposeB_=false, DataType data_type_=DataType::FLOAT) {
        memset(this, 0, sizeof(*this));
        this->transposeA = transposeA_;
        this->transposeB = transposeB_;
        this->data_type = data_type_;
    }
};

struct MatrixMulV1 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 751228537u;
    /*!
     * Specifies special computation modes, e.g. different combinations of
     * intermediate result data types.
     */
    enum class ComputeMode: uint32_t {
        //! No special requirements on the precision of intermediate results.
        DEFAULT = 0,
        /*!
         * Use Float32 accumulator and intermediate result. Only supported when
         * input and output is Float16.
         */
        FLOAT32 = 1
    };
    static MEGDNN_CONSTEXPR uint32_t COMPUTEMODE_NR_MEMBER = 2;
    union { struct {
    bool transposeA;
    bool transposeB;
    ComputeMode compute_mode;
    }; };
    MatrixMulV1(bool transposeA_=false, bool transposeB_=false, ComputeMode compute_mode_=ComputeMode::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->transposeA = transposeA_;
        this->transposeB = transposeB_;
        this->compute_mode = compute_mode_;
    }
};

struct MatrixMul {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2799239548u;
    using ComputeMode = MatrixMulV1::ComputeMode;
    static MEGDNN_CONSTEXPR uint32_t COMPUTEMODE_NR_MEMBER = 2;
    enum class Format: uint32_t {
        //! Normal matrix mul: (M, K) x (K, N) = (M, N)
        DEFAULT = 0,
        /*!
         * Split 4 from M and K, better for neon compute:(M/4, K/4, 4(k), 4(m))
         * x (K/4, N, 4(k)). if transposeA the layout is (K/4, M/4, 4(k), 4(m))
         * x (K/4, N, 4(k))
         */
        MK4 = 1,
        /*!
         * Split 8 from M and K, better for neon compute:(M/8, K/8, 8(k), 8(m))
         * x (K/8, N, 8(k)). if transposeA the layout is (K/8, M/8, 8(k), 8(m))
         * x (K/8, N, 8(k))
         */
        MK8 = 2,
        /*!
         * Split 4 from M and K, better for neon dotprod:M/4, K/4, 4(m), 4(k))
         * x (K/4, N, 4(k)). if transposeA the layout is (K/4, M/4, 4(m), 4(k))
         * x (K/4, N, 4(k))
         */
        MK4_DOT = 3
    };
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 4;
    union { struct {
    bool transposeA;
    bool transposeB;
    ComputeMode compute_mode;
    Format format;
    }; };
    MatrixMul(bool transposeA_=false, bool transposeB_=false, ComputeMode compute_mode_=ComputeMode::DEFAULT, Format format_=Format::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->transposeA = transposeA_;
        this->transposeB = transposeB_;
        this->compute_mode = compute_mode_;
        this->format = format_;
    }
};

//! winograd param used in convbias
struct Winograd {
    static MEGDNN_CONSTEXPR uint32_t TAG = 293958225u;
    using Format = MatrixMul::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 4;
    using ComputeMode = Convolution::ComputeMode;
    static MEGDNN_CONSTEXPR uint32_t COMPUTEMODE_NR_MEMBER = 2;
    union { struct {
    /*!
     * output block size, detail meaning see winograd in convbias, equals to
     * the meaning of m in F(m, r)
     */
    uint32_t output_block_size;
    Format format;
    ComputeMode compute_mode;
    }; };
    Winograd(uint32_t output_block_size_=0, Format format_=Format::DEFAULT, ComputeMode compute_mode_=ComputeMode::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->output_block_size = output_block_size_;
        this->format = format_;
        this->compute_mode = compute_mode_;
    }
};

struct SVD {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3024195679u;
    union { struct {
    /*!
     * Whether to compute the full-sized u and v or only the leading min(m, n)
     * singular vectors. Ignored if compute_uv is false.
     */
    bool full_matrices;
    /*!
     * Whether the left (u) and right (v) singular vectors will be computed and
     * outputted.
     */
    bool compute_uv;
    }; };
    SVD(bool full_matrices_=false, bool compute_uv_=true) {
        memset(this, 0, sizeof(*this));
        this->full_matrices = full_matrices_;
        this->compute_uv = compute_uv_;
    }
};

//! legacy reduce
struct ReduceV0 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1111565944u;
    enum class Mode: uint32_t {
        SUM = 0,
        //! sum of x * x for each element x
        SUM_SQR = 1,
        PRODUCT = 2,
        MIN = 3,
        MAX = 4
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 5;
    union { struct {
    Mode mode;
    /*!
     * axis along which reduction is performed; if -1 is given, reduce to given
     * target shape (only used in megbrain)
     */
    int32_t axis;
    }; };
    ReduceV0(Mode mode_=Mode::SUM, int32_t axis_=-1) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->axis = axis_;
    }
};

//! reduce along given axis
struct ReduceV1 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 229533304u;
    enum class Mode: uint32_t {
        SUM = 0,
        //! sum of x * x for each element x
        SUM_SQR = 1,
        PRODUCT = 2,
        MIN = 3,
        MAX = 4,
        MEAN = 5
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 6;
    enum class DataType: uint32_t {
        /*
        * input/output are the same data type, and the internal computation type would be chosen by the input/output dtypes and the reduction mode.
        * Currently, ```DEFAULT``` mode means:
        * 
        * +--------------------+-----------------------------------+-------------------+
        * | Input/Output DType | Mode                              | Computation DType |
        * +====================+===================================+===================+
        * | FLOAT32            | MIN/MAX/MEAN/SUM/SUM_SQR/PRODUCT  | FLOAT32           |
        * +--------------------+-----------------------------------+-------------------+
        * | FLOAT16            | MIN/MAX/MEAN/SUM/SUM_SQR/PRODUCT  | FLOAT16           |
        * +--------------------+-----------------------------------+-------------------+
        * | INT32              | MIN/MAX/MEAN/SUM/SUM_SQR/PRODUCT  | INT32             |
        * +--------------------+-----------------------------------+-------------------+
        * | INT8               | MIN/MAX/MEAN/SUM/SUM_SQR/PRODUCT  | INT8              |
        * +--------------------+-----------------------------------+-------------------+
        * | QuantizedS8        | MIN/MAX                           | QuantizedS8       |
        * +--------------------+-----------------------------------+-------------------+
        * | QuantizedS8        | MEAN/SUM                          | QuantizedS32      |
        * +--------------------+-----------------------------------+-------------------+
        * | Quantized8Asymm    | MIN/MAX                           | Quantized8Asymm   |
        * +--------------------+-----------------------------------+-------------------+
        * | Quantized8Asymm    | MEAN/SUM                          | QuantizedS32      |
        * +--------------------+-----------------------------------+-------------------+
        * 
        * 
        */
        DEFAULT = 0,
        /*!
         * Deprecated. This was replaced by FLOAT_O16xC32, and input's dtype
         * decided by actual input tensor.
         */
        FLOAT_IO16xC32 = 1,
        //! compute/output both are float32
        FLOAT_O32xC32 = 2,
        //! compute are float32, output float16
        FLOAT_O16xC32 = 3,
        //! input quint8, compute and output are qint32
        QUINT_I8xO32 = 4,
        //! input qint8, compute and output are qint32
        QINT_I8xO32 = 5
    };
    static MEGDNN_CONSTEXPR uint32_t DATATYPE_NR_MEMBER = 6;
    union { struct {
    Mode mode;
    /*!
     * axis along which reduction is performed; if -1 is given, reduce to given
     * target shape (only used in megbrain)
     */
    int32_t axis;
    DataType data_type;
    }; };
    ReduceV1(Mode mode_=Mode::SUM, int32_t axis_=-1, DataType data_type_=DataType::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->axis = axis_;
        this->data_type = data_type_;
    }
};

//! reduce along given axis
struct Reduce {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1246642972u;
    enum class Mode: uint32_t {
        SUM = 0,
        //! sum of x * x for each element x
        SUM_SQR = 1,
        PRODUCT = 2,
        MIN = 3,
        MAX = 4,
        MEAN = 5
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 6;
    enum class DataType: uint32_t {
        /*
        * input/output are the same data type, and the internal computation type would be chosen by the input/output dtypes and the reduction mode.
        * Currently, ```DEFAULT``` mode means:
        * 
        * +--------------------+-----------------------------------+-------------------+
        * | Input/Output DType | Mode                              | Computation DType |
        * +====================+===================================+===================+
        * | FLOAT32            | MIN/MAX/MEAN/SUM/SUM_SQR/PRODUCT  | FLOAT32           |
        * +--------------------+-----------------------------------+-------------------+
        * | FLOAT16            | MIN/MAX/MEAN/SUM/SUM_SQR/PRODUCT  | FLOAT16           |
        * +--------------------+-----------------------------------+-------------------+
        * | INT32              | MIN/MAX/MEAN/SUM/SUM_SQR/PRODUCT  | INT32             |
        * +--------------------+-----------------------------------+-------------------+
        * | INT8               | MIN/MAX/MEAN/SUM/SUM_SQR/PRODUCT  | INT8              |
        * +--------------------+-----------------------------------+-------------------+
        * | QuantizedS8        | MIN/MAX                           | QuantizedS8       |
        * +--------------------+-----------------------------------+-------------------+
        * | QuantizedS8        | MEAN/SUM                          | QuantizedS32      |
        * +--------------------+-----------------------------------+-------------------+
        * | Quantized8Asymm    | MIN/MAX                           | Quantized8Asymm   |
        * +--------------------+-----------------------------------+-------------------+
        * | Quantized8Asymm    | MEAN/SUM                          | QuantizedS32      |
        * +--------------------+-----------------------------------+-------------------+
        * 
        * 
        */
        DEFAULT = 0,
        /*!
         * Deprecated. This was replaced by FLOAT_O16xC32, and input's dtype
         * decided by actual input tensor.
         */
        FLOAT_IO16xC32 = 1,
        //! compute/output both are float32
        FLOAT_O32xC32 = 2,
        //! compute are float32, output float16
        FLOAT_O16xC32 = 3,
        //! input quint8, compute and output are qint32
        QUINT_I8xO32 = 4,
        //! input qint8, compute and output are qint32
        QINT_I8xO32 = 5
    };
    static MEGDNN_CONSTEXPR uint32_t DATATYPE_NR_MEMBER = 6;
    union { struct {
    Mode mode;
    /*!
     * axis along which reduction is performed; if INT_MAX is given, reduce to
     * given target shape (only used in megbrain)
     */
    int32_t axis;
    DataType data_type;
    }; };
    Reduce(Mode mode_=Mode::SUM, int32_t axis_=2147483647, DataType data_type_=DataType::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->axis = axis_;
        this->data_type = data_type_;
    }
};

//! calculate accumulated sum along given axis
struct CumsumV0 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1887071490u;
    union { struct {
    //! axis along which cumsum is performed
    int32_t axis;
    //! whether the current element is taken into account
    bool exclusive;
    //! whether the cumsum is forward or backward
    bool reverse;
    }; };
    CumsumV0(int32_t axis_=-1, bool exclusive_=true, bool reverse_=false) {
        memset(this, 0, sizeof(*this));
        this->axis = axis_;
        this->exclusive = exclusive_;
        this->reverse = reverse_;
    }
};

//! calculate accumulated sum along given axis
struct Cumsum {
    static MEGDNN_CONSTEXPR uint32_t TAG = 774078408u;
    union { struct {
    //! axis along which cumsum is performed, default with INT_MAX
    int32_t axis;
    //! whether the current element is taken into account
    bool exclusive;
    //! whether the cumsum is forward or backward
    bool reverse;
    }; };
    Cumsum(int32_t axis_=2147483647, bool exclusive_=true, bool reverse_=false) {
        memset(this, 0, sizeof(*this));
        this->axis = axis_;
        this->exclusive = exclusive_;
        this->reverse = reverse_;
    }
};

struct CondTake {
    static MEGDNN_CONSTEXPR uint32_t TAG = 734043720u;
    enum class Mode: uint32_t {
        //! take if ``abs(data-val)<eps``
        EQ = 0,
        //! take if ``abs(data-val)>=eps``
        NEQ = 1,
        //! take if ``data<val``
        LT = 2,
        //! take if ``data<=val``
        LEQ = 3,
        //! take if ``data>val``
        GT = 4,
        //! take if ``data>=val``
        GEQ = 5
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 6;
    union { struct {
    Mode mode;
    /*!
     * the value to be compared with; note that for integer data, val is also
     * converted to int
     */
    float val;
    //! used for float equality comparison
    float eps;
    }; };
    CondTake(Mode mode_=Mode::EQ, float val_=0, float eps_=1e-06) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->val = val_;
        this->eps = eps_;
    }
};

struct Argsort {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2993639216u;
    enum class Order: uint32_t {
        ASCENDING = 0,
        DESCENDING = 1
    };
    static MEGDNN_CONSTEXPR uint32_t ORDER_NR_MEMBER = 2;
    union { struct {
    Order order;
    }; };
    Argsort(Order order_=Order::ASCENDING) {
        memset(this, 0, sizeof(*this));
        this->order = order_;
    }
};

struct IndexingRemap {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3137589921u;
    union { struct {
    /*!
     * Whether no two dst element maps to the same src element. Enabling this
     * option can accelerate gradient operator since atomic adding operations
     * could be avoided.
     */
    bool is_non_overlapping;
    }; };
    IndexingRemap(bool is_non_overlapping_=false) {
        memset(this, 0, sizeof(*this));
        this->is_non_overlapping = is_non_overlapping_;
    }
};

struct Sleep {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3563502837u;
    union { struct {
    //! time to sleep in seconds
    float time;
    }; };
    Sleep(float time_=0) {
        memset(this, 0, sizeof(*this));
        this->time = time_;
    }
};

struct Linspace {
    static MEGDNN_CONSTEXPR uint32_t TAG = 616024820u;
    union { struct {
    //! Whether stop is included in the generated tensor
    bool endpoint;
    }; };
    Linspace(bool endpoint_=true) {
        memset(this, 0, sizeof(*this));
        this->endpoint = endpoint_;
    }
};

struct LinspaceFull {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1318409902u;
    union { struct {
    //! The first val.
    double start;
    //! The last val.
    double stop;
    //! Whether stop is included in the generated tensor
    bool endpoint;
    }; };
    LinspaceFull(double start_=0, double stop_=1, bool endpoint_=true) {
        memset(this, 0, sizeof(*this));
        this->start = start_;
        this->stop = stop_;
        this->endpoint = endpoint_;
    }
};

struct Eye {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3471011925u;
    union { struct {
    /*!
     * Index of the diagonal: 0 (the default) refers to the main diagonal, a
     * positive value refers to an upper diagonal, and a negative value to a
     * lower diagonal.
     */
    int32_t k;
    //! data type of output value
    DTypeEnum dtype;
    }; };
    Eye(int32_t k_=0, DTypeEnum dtype_=DTypeEnum::Float32) {
        memset(this, 0, sizeof(*this));
        this->k = k_;
        this->dtype = dtype_;
    }
};

struct UniformRNG {
    static MEGDNN_CONSTEXPR uint32_t TAG = 379446398u;
    union { struct {
    alignas(sizeof(uint64_t)) uint64_t seed;
    }; };
    UniformRNG(uint64_t seed_=0) {
        memset(this, 0, sizeof(*this));
        this->seed = seed_;
    }
};

struct GaussianRNG {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1035599260u;
    union { struct {
    alignas(sizeof(uint64_t)) uint64_t seed;
    float mean;
    float std;
    }; };
    GaussianRNG(uint64_t seed_=0, float mean_=0, float std_=1) {
        memset(this, 0, sizeof(*this));
        this->seed = seed_;
        this->mean = mean_;
        this->std = std_;
    }
};

struct Flip {
    static MEGDNN_CONSTEXPR uint32_t TAG = 608817463u;
    union { struct {
    bool vertical;
    bool horizontal;
    }; };
    Flip(bool vertical_=false, bool horizontal_=false) {
        memset(this, 0, sizeof(*this));
        this->vertical = vertical_;
        this->horizontal = horizontal_;
    }
};

struct Rotate {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3277929239u;
    union { struct {
    bool clockwise;
    }; };
    Rotate(bool clockwise_=true) {
        memset(this, 0, sizeof(*this));
        this->clockwise = clockwise_;
    }
};

struct ROICopy {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2619888986u;
    union { struct {
    uint32_t row_from;
    uint32_t row_to;
    uint32_t col_from;
    uint32_t col_to;
    }; };
    ROICopy(uint32_t row_from_=0, uint32_t row_to_=0, uint32_t col_from_=0, uint32_t col_to_=0) {
        memset(this, 0, sizeof(*this));
        this->row_from = row_from_;
        this->row_to = row_to_;
        this->col_from = col_from_;
        this->col_to = col_to_;
    }
};

struct CvtColor {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1364268694u;
    enum class Mode: uint32_t {
        RGB2GRAY = 0,
        RGB2YUV = 1,
        YUV2RGB = 2,
        GRAY2RGB = 3,
        RGBA2RGB = 4,
        RGBA2BGR = 5,
        RGBA2GRAY = 6,
        RGB2BGR = 7,
        BGR2GRAY = 8,
        BGR2RGB = 9,
        //! For historical reasons, referred to as YCC by opencv
        YUV2GRAY_NV21 = 10,
        YUV2RGB_NV21 = 11,
        YUV2BGR_NV21 = 12,
        YUV2GRAY_NV12 = 13,
        YUV2RGB_NV12 = 14,
        YUV2BGR_NV12 = 15,
        YUV2GRAY_YV12 = 16,
        YUV2RGB_YV12 = 17,
        YUV2BGR_YV12 = 18,
        YUV2GRAY_YU12 = 19,
        YUV2RGB_YU12 = 20,
        YUV2BGR_YU12 = 21,
        YCrCb2RGB = 22,
        YCrCb2BGR = 23,
        //! BT601 yuv format, referred to as YUV by opencv
        BT601_YUV2RGB_NV21 = 24,
        BT601_YUV2BGR_NV21 = 25,
        BT601_YUV2RGB_NV12 = 26,
        BT601_YUV2BGR_NV12 = 27,
        BT601_YUV2RGB_YV12 = 28,
        BT601_YUV2BGR_YV12 = 29,
        BT601_YUV2RGB_YU12 = 30,
        BT601_YUV2BGR_YU12 = 31,
        BT601_YUV2GRAY_NV21 = YUV2GRAY_NV21,
        BT601_YUV2GRAY_NV12 = YUV2GRAY_NV12,
        BT601_YUV2GRAY_YV12 = YUV2GRAY_YV12,
        BT601_YUV2GRAY_YU12 = YUV2GRAY_YU12,
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 32;
    union { struct {
    Mode mode;
    }; };
    CvtColor(Mode mode_=Mode::RGB2GRAY) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
    }
};

struct WarpAffineV0 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1507366749u;
    using InterpolationMode = WarpPerspective::InterpolationMode;
    static MEGDNN_CONSTEXPR uint32_t INTERPOLATIONMODE_NR_MEMBER = 5;
    using BorderMode = WarpPerspective::BorderMode;
    static MEGDNN_CONSTEXPR uint32_t BORDERMODE_NR_MEMBER = 7;
    union { struct {
    InterpolationMode imode;
    BorderMode border_mode;
    //! used for CONSTANT bmode
    float border_val;
    }; };
    WarpAffineV0(InterpolationMode imode_=InterpolationMode::LINEAR, BorderMode border_mode_=BorderMode::REPLICATE, float border_val_=.0f) {
        memset(this, 0, sizeof(*this));
        this->imode = imode_;
        this->border_mode = border_mode_;
        this->border_val = border_val_;
    }
};

struct WarpAffine {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3653482822u;
    using InterpolationMode = WarpPerspective::InterpolationMode;
    static MEGDNN_CONSTEXPR uint32_t INTERPOLATIONMODE_NR_MEMBER = 5;
    using BorderMode = WarpPerspective::BorderMode;
    static MEGDNN_CONSTEXPR uint32_t BORDERMODE_NR_MEMBER = 7;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    InterpolationMode imode;
    BorderMode border_mode;
    //! used for CONSTANT bmode
    float border_val;
    Format format;
    }; };
    WarpAffine(InterpolationMode imode_=InterpolationMode::LINEAR, BorderMode border_mode_=BorderMode::REPLICATE, float border_val_=.0f, Format format_=Format::NHWC) {
        memset(this, 0, sizeof(*this));
        this->imode = imode_;
        this->border_mode = border_mode_;
        this->border_val = border_val_;
        this->format = format_;
    }
};

struct GaussianBlur {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3213149798u;
    using BorderMode = WarpPerspective::BorderMode;
    static MEGDNN_CONSTEXPR uint32_t BORDERMODE_NR_MEMBER = 7;
    union { struct {
    BorderMode border_mode;
    uint32_t kernel_height;
    uint32_t kernel_width;
    float sigma_x;
    float sigma_y;
    }; };
    GaussianBlur(BorderMode border_mode_=BorderMode::REPLICATE, uint32_t kernel_height_=0, uint32_t kernel_width_=0, float sigma_x_=0.f, float sigma_y_=0.f) {
        memset(this, 0, sizeof(*this));
        this->border_mode = border_mode_;
        this->kernel_height = kernel_height_;
        this->kernel_width = kernel_width_;
        this->sigma_x = sigma_x_;
        this->sigma_y = sigma_y_;
    }
};

struct ResizeV0 {
    static MEGDNN_CONSTEXPR uint32_t TAG = 693559402u;
    using InterpolationMode = WarpPerspective::InterpolationMode;
    static MEGDNN_CONSTEXPR uint32_t INTERPOLATIONMODE_NR_MEMBER = 5;
    union { struct {
    InterpolationMode imode;
    }; };
    ResizeV0(InterpolationMode imode_=InterpolationMode::LINEAR) {
        memset(this, 0, sizeof(*this));
        this->imode = imode_;
    }
};

struct Resize {
    static MEGDNN_CONSTEXPR uint32_t TAG = 31792670u;
    using InterpolationMode = WarpPerspective::InterpolationMode;
    static MEGDNN_CONSTEXPR uint32_t INTERPOLATIONMODE_NR_MEMBER = 5;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    InterpolationMode imode;
    Format format;
    }; };
    Resize(InterpolationMode imode_=InterpolationMode::LINEAR, Format format_=Format::NHWC) {
        memset(this, 0, sizeof(*this));
        this->imode = imode_;
        this->format = format_;
    }
};

struct Remap {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3129834253u;
    using InterpolationMode = WarpPerspective::InterpolationMode;
    static MEGDNN_CONSTEXPR uint32_t INTERPOLATIONMODE_NR_MEMBER = 5;
    using BorderMode = WarpPerspective::BorderMode;
    static MEGDNN_CONSTEXPR uint32_t BORDERMODE_NR_MEMBER = 7;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    InterpolationMode imode;
    BorderMode border_type;
    Format format;
    float scalar;
    }; };
    Remap(InterpolationMode imode_=InterpolationMode::LINEAR, BorderMode border_type_=BorderMode::REPLICATE, Format format_=Format::NHWC, float scalar_=0.f) {
        memset(this, 0, sizeof(*this));
        this->imode = imode_;
        this->border_type = border_type_;
        this->format = format_;
        this->scalar = scalar_;
    }
};

struct Convolution3D {
    static MEGDNN_CONSTEXPR uint32_t TAG = 3027053684u;
    enum class Mode: uint32_t {
        CROSS_CORRELATION = 0,
        CONVOLUTION = 1
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    enum class Sparse: uint32_t {
        /*!
         * dense convolution: filter shape should be [oc, ic, spatial...] if
         * format is NCDHW, [oc, spatial..., ic] if format is NDHWC
         */
        DENSE = 0,
        /*!
         * group convolution: filter shape should be [group, oc_per_group,
         * ic_per_group, spatial...] if format is NCDHW, [group, oc_per_group,
         * spatial..., ic_per_group] if format is NDHWC
         */
        GROUP = 1
    };
    static MEGDNN_CONSTEXPR uint32_t SPARSE_NR_MEMBER = 2;
    enum class DataType: uint32_t {
        //! input/output both float32/float16
        FLOAT = 0,
        //! input/output both float16, the internal compute is float32
        FLOAT_IO16xC32 = 1
    };
    static MEGDNN_CONSTEXPR uint32_t DATATYPE_NR_MEMBER = 2;
    enum class Format: uint32_t {
        NCDHW = 0,
        NDHWC = 1
    };
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 2;
    union { struct {
    Mode mode;
    //! padding on one side on the first dimension
    uint32_t pad_d;
    //! padding on one side on the second dimension
    uint32_t pad_h;
    //! padding on one side on the third dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_d;
    //! kernel stride on the second dimension
    uint32_t stride_h;
    //! kernel stride on the third dimension
    uint32_t stride_w;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the first
     * dimension
     */
    uint32_t dilate_d;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_h;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the third
     * dimension
     */
    uint32_t dilate_w;
    Sparse sparse;
    DataType data_type;
    Format format;
    }; };
    Convolution3D(Mode mode_=Mode::CROSS_CORRELATION, uint32_t pad_d_=0, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_d_=1, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t dilate_d_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1, Sparse sparse_=Sparse::DENSE, DataType data_type_=DataType::FLOAT, Format format_=Format::NCDHW) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->pad_d = pad_d_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_d = stride_d_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->dilate_d = dilate_d_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
        this->sparse = sparse_;
        this->data_type = data_type_;
        this->format = format_;
    }
};

struct Conv3DBias {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2060241070u;
    enum class NonlineMode: uint32_t {
        IDENTITY = 0,
        RELU = 1,
        SIGMOID = 2
    };
    static MEGDNN_CONSTEXPR uint32_t NONLINEMODE_NR_MEMBER = 3;
    using Mode = Convolution3D::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    union { struct {
    NonlineMode nonlineMode;
    Mode mode;
    uint32_t pad_d;
    uint32_t pad_h;
    uint32_t pad_w;
    uint32_t stride_d;
    uint32_t stride_h;
    uint32_t stride_w;
    }; };
    Conv3DBias(NonlineMode nonlineMode_=NonlineMode::IDENTITY, Mode mode_=Mode::CROSS_CORRELATION, uint32_t pad_d_=0, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_d_=1, uint32_t stride_h_=1, uint32_t stride_w_=0) {
        memset(this, 0, sizeof(*this));
        this->nonlineMode = nonlineMode_;
        this->mode = mode_;
        this->pad_d = pad_d_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_d = stride_d_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
    }
};

struct SeparableConv3D {
    static MEGDNN_CONSTEXPR uint32_t TAG = 840825259u;
    using Mode = Convolution3D::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    enum class BorderMode: uint32_t {
        BORDER_REPLICATE = 0,
        BORDER_REFLECT = 1,
        BORDER_REFLECT_101 = 2,
        BORDER_WRAP = 3,
        BORDER_CONSTANT = 4,
        BORDER_TRANSPARENT = 5,
        BORDER_ISOLATED = 6
    };
    static MEGDNN_CONSTEXPR uint32_t BORDERMODE_NR_MEMBER = 7;
    union { struct {
    Mode mode;
    BorderMode borderMode;
    bool is_symm_kernel;
    uint32_t pad_d;
    uint32_t pad_h;
    uint32_t pad_w;
    uint32_t stride_d;
    uint32_t stride_h;
    uint32_t stride_w;
    uint32_t ksize_d;
    uint32_t ksize_h;
    uint32_t ksize_w;
    uint32_t anchor_d;
    uint32_t anchor_h;
    uint32_t anchor_w;
    }; };
    SeparableConv3D(Mode mode_=Mode::CROSS_CORRELATION, BorderMode borderMode_=BorderMode::BORDER_REPLICATE, bool is_symm_kernel_=true, uint32_t pad_d_=0, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_d_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t ksize_d_=0, uint32_t ksize_h_=3, uint32_t ksize_w_=3, uint32_t anchor_d_=0, uint32_t anchor_h_=1, uint32_t anchor_w_=1) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->borderMode = borderMode_;
        this->is_symm_kernel = is_symm_kernel_;
        this->pad_d = pad_d_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_d = stride_d_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->ksize_d = ksize_d_;
        this->ksize_h = ksize_h_;
        this->ksize_w = ksize_w_;
        this->anchor_d = anchor_d_;
        this->anchor_h = anchor_h_;
        this->anchor_w = anchor_w_;
    }
};

struct TopK {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1128623317u;
    enum class Mode: uint32_t {
        //! only the value of the k'th element would be computed
        KTH_ONLY = 0,
        /*!
         * all the top-k values and corresponding indices would be computed; no
         * order is guaranteed
         */
        VALUE_IDX_NOSORT = 1,
        //! all the top-k values and corresponding indices sorted
        VALUE_IDX_SORTED = 2
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 3;
    union { struct {
    Mode mode;
    }; };
    TopK(Mode mode_=Mode::KTH_ONLY) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
    }
};

//! Change the tensor layout format
struct RelayoutFormat {
    static MEGDNN_CONSTEXPR uint32_t TAG = 448628553u;
    /*
    * Relayout mode.
    * 
    * **Naming conventions**
    * 
    * 1. ``A_B`` means change from layout format ``A`` to ``B``.
    * 2. ``INTER_WEIGHT_xx`` means relayout the weight for faster processing by
    *    :attr:`Convolution.Format.NHWCD4` convolutions.
    * 3. A suffix of ``I`` means ``Image2DPack4TensorFormat`` tensor format is used
    *    for faster processing on GPUs.
    * 
    * **Layout definitions**
    * 
    * * ``NCHW`` layout: ``{N, C, H, W}``
    * * ``NHWC`` layout: ``{N, H, W, C}``
    * * ``NHWCD4`` layout: ``{N, H, (C + 3) / 4, W, 4}``
    * * ``NHWCD4I`` layout: with ``align_axis = 2``
    * * ``NCHW4`` layout: ``{N, C/4, H, W, 4}``
    * * ``NCHW88`` layout: ``{N, C/8, H, W, 8}``
    * * ``CHWN4`` layout: ``{C/4, H, W, N, 4}``
    * 
    * **Float weight transformation definitions**
    * 
    * +---------------+---------------------------------+--------------------+--------------------------------------+------+
    * | Sparsity Type | Input Layout                    | Input Req          | Output Layout                        | Axis |
    * +===============+=================================+====================+======================================+======+
    * | DENSE         | ``{OC, IC, FH, FW}``            | ``OC % 4 == 0``    | ``{OC/4, FH, FW, IC, 4}``            | 3    |
    * +---------------+---------------------------------+--------------------+--------------------------------------+------+
    * | GROUP         | ``{GROUP, OCPG, ICPG, FH, FW}`` | ``OCPG % 4 == 0``  | ``{GROUP, OCPG/4, FH, FW, ICPG, 4}`` | 4    |
    * |               |                                 | ``ICPG % 4 == 0``  |                                      |      |
    * +---------------+---------------------------------+--------------------+--------------------------------------+------+
    * | CHAN          | ``{GROUP, 1, 1, FH, FW}``       | ``GROUP % 4 == 0`` | ``{GROUP / 4, 1, FH ,FW, 4}``        | 1    |
    * +---------------+---------------------------------+--------------------+--------------------------------------+------+
    * 
    * **Float weight transformation nchw88 definitions**
    * 
    * +---------------+---------------------------------+--------------------+--------------------------------------+
    * | Sparsity Type | Input Layout                    | Input Req          | Output Layout                        |
    * +===============+=================================+====================+======================================+
    * | DENSE         | ``{OC, IC, FH, FW}``            | ``OC % 8 == 0``    |``{OC/8, IC/8 ,FH, FW, 8(IC), 8(OC)}``|
    * |               |                                 | ``IC % 8 == 0``    |                                      |
    * +---------------+---------------------------------+--------------------+--------------------------------------+
    * | GROUP         | ``{GROUP, OCPG, ICPG, FH, FW}`` | ``OCPG % 8 == 0``  | ``{GROUP, OCPG/8, ICPG/8 FH, FW,     |
    * |               |                                 | ``ICPG % 8 == 0``  |  8(ICPG), 8(OCPG)} ``                |
    * +---------------+---------------------------------+--------------------+--------------------------------------+
    * | CHAN          | ``{GROUP, 1, 1, FH, FW}``       | ``GROUP % 8 == 0`` | ``{GROUP / 8, 1, FH ,FW, 8}``        |
    * +---------------+---------------------------------+--------------------+--------------------------------------+
    * 
    * **Int8(DOT) weight transformation definitions**
    * 
    * +---------------+---------------------------------+--------------------+------------------------------------------+------+
    * | Sparsity Type | Input Layout                    | Input Req          | Output Layout                            | Axis |
    * +===============+=================================+====================+==========================================+======+
    * | DENSE         | ``{OC, IC, FH, FW}``            | ``OC % 4 == 0``    | ``{OC/4, FH, FW, IC/4, 4, 4}`            | 3    |
    * +---------------+---------------------------------+--------------------+------------------------------------------+------+
    * | GROUP         | ``{GROUP, OCPG, ICPG, FH, FW}`` | ``OCPG % 4 == 0``  | ``{GROUP, OCPG/4, FH, FW, ICPG/4, 4, 4}``| 4    |
    * |               |                                 | ``ICPG % 4 == 0``  |                                          |      |
    * +---------------+---------------------------------+--------------------+------------------------------------------+------+
    * 
    * Note: the axis column means the corresponding ``align_axis`` for image format
    * when the ``I`` suffix is present.
    * 
    * 
    */
    enum class Mode: uint32_t {
        NHWC_NHWCD4 = 0,
        NHWCD4_NHWC = 1,
        NHWC_NHWCD4I = 2,
        NCHW_NHWCD4 = 3,
        NCHW_NHWCD4I = 4,
        NHWCD4I_NCHW = 5,
        NHWCD4_NCHW = 6,
        INTER_WEIGHT_DENSE = 7,
        INTER_WEIGHT_DENSEI = 8,
        INTER_WEIGHT_GROUP = 9,
        INTER_WEIGHT_GROUPI = 10,
        INTER_WEIGHT_CHAN = 11,
        INTER_WEIGHT_CHANI = 12,
        INTER_WEIGHT_DENSEI_DOT = 13,
        INTER_WEIGHT_GROUPI_DOT = 14,
        NCHW4_CHWN4 = 15,
        CHWN4_NCHW4 = 16,
        NCHW_NCHW88_CONV_DENSE_WEIGHT = 17,
        NCHW_NCHW88_CONV_CHAN_WEIGHT = 18,
        NCHW_NCHW88_CONV_GROUP_WEIGHT = 19,
        NCHW_NCHW88 = 20,
        NCHW88_NCHW = 21,
        NCHW_NCHW4_IC_SMALL = 22,
        NCHW_NCHW4_IC_SMALL_CONV_DENSE_WEIGHT = 23
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 24;
    union { struct {
    Mode mode;
    }; };
    RelayoutFormat(Mode mode_=Mode::NHWC_NHWCD4) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
    }
};

struct SeparableFilter {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2641095096u;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    using BorderMode = WarpPerspective::BorderMode;
    static MEGDNN_CONSTEXPR uint32_t BORDERMODE_NR_MEMBER = 7;
    union { struct {
    Format format;
    BorderMode borderMode;
    bool is_symm_kernel;
    uint32_t ksize_h;
    uint32_t ksize_w;
    uint32_t anchor_h;
    uint32_t anchor_w;
    }; };
    SeparableFilter(Format format_=Format::NCHW, BorderMode borderMode_=BorderMode::REPLICATE, bool is_symm_kernel_=true, uint32_t ksize_h_=3, uint32_t ksize_w_=3, uint32_t anchor_h_=1, uint32_t anchor_w_=1) {
        memset(this, 0, sizeof(*this));
        this->format = format_;
        this->borderMode = borderMode_;
        this->is_symm_kernel = is_symm_kernel_;
        this->ksize_h = ksize_h_;
        this->ksize_w = ksize_w_;
        this->anchor_h = anchor_h_;
        this->anchor_w = anchor_w_;
    }
};

//! Local share convolution
struct LocalShare {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2834312669u;
    using Mode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    using Sparse = ConvolutionV0::Sparse;
    static MEGDNN_CONSTEXPR uint32_t SPARSE_NR_MEMBER = 2;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    using ComputeMode = Convolution::ComputeMode;
    static MEGDNN_CONSTEXPR uint32_t COMPUTEMODE_NR_MEMBER = 2;
    union { struct {
    Mode mode;
    //! padding on one side on the first dimension
    uint32_t pad_h;
    //! padding on one side on the second dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_h;
    //! kernel stride on the second dimension
    uint32_t stride_w;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_h;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_w;
    //! spatial groups on the first dimension
    uint32_t spatial_groups_h;
    //! spatial groups on the second dimension
    uint32_t spatial_groups_w;
    Sparse sparse;
    Format format;
    ComputeMode computeMode;
    }; };
    LocalShare(Mode mode_=Mode::CROSS_CORRELATION, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1, uint32_t spatial_groups_h_=1, uint32_t spatial_groups_w_=1, Sparse sparse_=Sparse::DENSE, Format format_=Format::NCHW, ComputeMode computeMode_=ComputeMode::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
        this->spatial_groups_h = spatial_groups_h_;
        this->spatial_groups_w = spatial_groups_w_;
        this->sparse = sparse_;
        this->format = format_;
        this->computeMode = computeMode_;
    }
};

struct ROIAlign {
    static MEGDNN_CONSTEXPR uint32_t TAG = 2693062405u;
    enum class Mode: uint32_t {
        MAX = 0,
        AVERAGE = 1
    };
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    union { struct {
    Mode mode;
    Format format;
    float spatial_scale;
    float offset;
    uint32_t pooled_height;
    uint32_t pooled_width;
    uint32_t sample_height;
    uint32_t sample_width;
    }; };
    ROIAlign(Mode mode_=Mode::MAX, Format format_=Format::NCHW, float spatial_scale_=1.0, float offset_=0.0, uint32_t pooled_height_=1, uint32_t pooled_width_=1, uint32_t sample_height_=2, uint32_t sample_width_=2) {
        memset(this, 0, sizeof(*this));
        this->mode = mode_;
        this->format = format_;
        this->spatial_scale = spatial_scale_;
        this->offset = offset_;
        this->pooled_height = pooled_height_;
        this->pooled_width = pooled_width_;
        this->sample_height = sample_height_;
        this->sample_width = sample_width_;
    }
};

struct DeformablePSROIPooling {
    static MEGDNN_CONSTEXPR uint32_t TAG = 378949344u;
    union { struct {
    bool no_trans;
    float spatial_scale;
    float trans_std;
    //! height of pooling output
    uint32_t pooled_h;
    //! width of pooling output
    uint32_t pooled_w;
    //! size of each deformable part
    uint32_t part_size;
    //! sample count of each bbox
    uint32_t sample_per_part;
    }; };
    DeformablePSROIPooling(bool no_trans_=true, float spatial_scale_=1, float trans_std_=1, uint32_t pooled_h_=1, uint32_t pooled_w_=1, uint32_t part_size_=1, uint32_t sample_per_part_=1) {
        memset(this, 0, sizeof(*this));
        this->no_trans = no_trans_;
        this->spatial_scale = spatial_scale_;
        this->trans_std = trans_std_;
        this->pooled_h = pooled_h_;
        this->pooled_w = pooled_w_;
        this->part_size = part_size_;
        this->sample_per_part = sample_per_part_;
    }
};

//! Batch convolution (unshare weights on the batch dimension)
struct BatchConvBias {
    static MEGDNN_CONSTEXPR uint32_t TAG = 1513184730u;
    using NonlineMode = ConvBiasV0::NonlineMode;
    static MEGDNN_CONSTEXPR uint32_t NONLINEMODE_NR_MEMBER = 4;
    using Mode = ConvolutionV0::Mode;
    static MEGDNN_CONSTEXPR uint32_t MODE_NR_MEMBER = 2;
    using Sparse = ConvolutionV0::Sparse;
    static MEGDNN_CONSTEXPR uint32_t SPARSE_NR_MEMBER = 2;
    using Format = ConvolutionV0::Format;
    static MEGDNN_CONSTEXPR uint32_t FORMAT_NR_MEMBER = 13;
    using ComputeMode = Convolution::ComputeMode;
    static MEGDNN_CONSTEXPR uint32_t COMPUTEMODE_NR_MEMBER = 2;
    union { struct {
    NonlineMode nonlineMode;
    Mode mode;
    //! padding on one side on the first dimension
    uint32_t pad_h;
    //! padding on one side on the second dimension
    uint32_t pad_w;
    //! kernel stride on the first dimension
    uint32_t stride_h;
    //! kernel stride on the second dimension
    uint32_t stride_w;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_h;
    /*!
     * dilation (i.e. size of each zero-padded kernel block) on the second
     * dimension
     */
    uint32_t dilate_w;
    Sparse sparse;
    Format format;
    ComputeMode compute_mode;
    }; };
    BatchConvBias(NonlineMode nonlineMode_=NonlineMode::IDENTITY, Mode mode_=Mode::CROSS_CORRELATION, uint32_t pad_h_=0, uint32_t pad_w_=0, uint32_t stride_h_=1, uint32_t stride_w_=1, uint32_t dilate_h_=1, uint32_t dilate_w_=1, Sparse sparse_=Sparse::DENSE, Format format_=Format::NCHW, ComputeMode compute_mode_=ComputeMode::DEFAULT) {
        memset(this, 0, sizeof(*this));
        this->nonlineMode = nonlineMode_;
        this->mode = mode_;
        this->pad_h = pad_h_;
        this->pad_w = pad_w_;
        this->stride_h = stride_h_;
        this->stride_w = stride_w_;
        this->dilate_h = dilate_h_;
        this->dilate_w = dilate_w_;
        this->sparse = sparse_;
        this->format = format_;
        this->compute_mode = compute_mode_;
    }
};

} // namespace megdnn
} // namespace param
// vim: syntax=cpp.doxygen
